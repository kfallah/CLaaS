services:
  # ------------------------------------------------------------------
  # claas-api: Feedback API + inference proxy in Tinker execution mode
  # ------------------------------------------------------------------
  claas-api:
    build:
      context: ..
      dockerfile: docker/Dockerfile.tinker-claas-api
    environment:
      - CLAAS_STORAGE_BACKEND=local_fs
      - CLAAS_DISTILL_EXECUTION_MODE=tinker
      - CLAAS_BASE_MODEL_ID=${MODEL:-gpt-oss/GPT-OSS-120B}
      - CLAAS_TINKER_API_KEY=${TINKER_API_KEY:?set TINKER_API_KEY}
      - CLAAS_TINKER_BASE_MODEL=${MODEL:-gpt-oss/GPT-OSS-120B}
      - CLAAS_TINKER_STATE_PATH=/data/tinker_state.json
      - WAIT_FOR_BACKEND=0
    volumes:
      - claas-feedback-logs:/app/data/feedback
      - tinker-state:/data
    ports:
      - "${CLAAS_API_PORT:-8080}:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/"]
      interval: 10s
      timeout: 5s
      start_period: 20s
      retries: 20

  # ------------------------------------------------------------------
  # init: one-shot container â€” initialize Tinker LoRA + OpenClaw config
  # ------------------------------------------------------------------
  init:
    build:
      context: ..
      dockerfile: docker/Dockerfile.tinker-init
    environment:
      - CLAAS_DISTILL_EXECUTION_MODE=tinker
      - LORA_NAME=${LORA_NAME:-openclaw/assistant}
      - MODEL=${MODEL:-gpt-oss/GPT-OSS-120B}
      - VLLM_BASE_URL=http://claas-api:8080/v1
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN:-}
      - OPENCLAW_HOME=/openclaw-config
      - CLAAS_API_URL=http://claas-api:8080
      - FEEDBACK_BATCH_SIZE=${FEEDBACK_BATCH_SIZE:-4}
    volumes:
      - openclaw-config:/openclaw-config
      - ../plugins/claas-feedback:/app/plugins/claas-feedback:ro
    depends_on:
      claas-api:
        condition: service_healthy

  # ------------------------------------------------------------------
  # openclaw: Telegram bot gateway (uses CLaaS API for inference)
  # ------------------------------------------------------------------
  openclaw:
    build:
      context: ..
      dockerfile: docker/Dockerfile.tinker-openclaw
    environment:
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN:-}
      - VLLM_BASE_URL=http://claas-api:8080
      - BACKEND_HEALTH_URL=http://claas-api:8080/v1/models
    volumes:
      - openclaw-config:/home/node/.openclaw
    ports:
      - "${OPENCLAW_PORT:-18789}:18789"
    depends_on:
      claas-api:
        condition: service_healthy
      init:
        condition: service_completed_successfully

volumes:
  openclaw-config:
  claas-feedback-logs:
  tinker-state:
