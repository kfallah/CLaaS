services:
  # ==================== LOCAL PROFILE ====================

  vllm:
    profiles: [local]
    image: vllm/vllm-openai:v0.15.1
    entrypoint: ["/scripts/start_vllm_qwen3_8b.sh"]
    environment:
      - MODEL=${MODEL}
      - HOST=0.0.0.0
      - API_KEY=sk-local
      - SERVED_MODEL_NAMES=qwen3-8b
      - MAX_MODEL_LEN=${MAX_MODEL_LEN}
      - GPU_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION}
      - ENABLE_SLEEP_MODE=1
      - VLLM_SERVER_DEV_MODE=1
      - VLLM_ALLOW_RUNTIME_LORA_UPDATING=1
      - ENABLE_AUTO_TOOL_CHOICE=1
      - TOOL_CALL_PARSER=qwen3_xml
      - LORA_ROOT=/loras
      - LORA_ALIAS_FILE=/loras/.aliases.json
      - INCLUDE_ALIAS_LORAS=1
      - HF_TOKEN=${HF_TOKEN:-}
    volumes:
      - ./scripts/start_vllm_qwen3_8b.sh:/scripts/start_vllm_qwen3_8b.sh:ro
      - hf-cache:/root/.cache/huggingface
      - lora-storage:/loras
    ports:
      - "${VLLM_PORT:-8000}:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 15s
      timeout: 10s
      start_period: 180s
      retries: 80
    depends_on:
      init-local:
        condition: service_completed_successfully

  claas-api:
    profiles: [local]
    build:
      context: ..
      dockerfile: docker/Dockerfile.claas-api
    command:
      [
        "local",
        "base_model_id=${MODEL}",
        "vllm_base_url=http://vllm:8000",
      ]
    environment:
      - VLLM_API_KEY=sk-local
      - HF_TOKEN=${HF_TOKEN:-}
      - HF_HOME=/root/.cache/huggingface
    volumes:
      - lora-storage:/loras
      - hf-cache:/root/.cache/huggingface
      - claas-feedback-logs:/app/data/feedback
    ports:
      - "${CLAAS_API_PORT:-8080}:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      vllm:
        condition: service_healthy

  init-local:
    profiles: [local]
    build:
      context: ..
      dockerfile: docker/Dockerfile.init
    command:
      [
        "--config-name",
        "local",
        "--base-model=${MODEL}",
        "--vllm-base-url=http://vllm:8000/v1",
        "--lora-root=/loras",
      ]
    environment:
      - LORA_NAME=${LORA_NAME}
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN}
      - OPENCLAW_HOME=/openclaw-config
      - CLAAS_API_URL=http://claas-api:8080
      - HF_HOME=/root/.cache/huggingface
      - HF_TOKEN=${HF_TOKEN:-}
    volumes:
      - lora-storage:/loras
      - openclaw-config:/openclaw-config
      - hf-cache:/root/.cache/huggingface
      - ../plugins/claas-feedback:/app/plugins/claas-feedback:ro

  openclaw-local:
    profiles: [local]
    build:
      context: ..
      dockerfile: docker/Dockerfile.openclaw
    environment:
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN:-}
      - VLLM_BASE_URL=http://claas-api:8080
    volumes:
      - openclaw-config:/home/node/.openclaw
    ports:
      - "${OPENCLAW_PORT:-18789}:18789"
    depends_on:
      claas-api:
        condition: service_started
      init-local:
        condition: service_completed_successfully

  # ==================== TINKER PROFILE ====================

  claas-api-tinker:
    profiles: [tinker]
    build:
      context: ..
      dockerfile: docker/Dockerfile.tinker
      target: api
    command:
      [
        "tinker",
        "tinker_base_model=${MODEL}",
        "tinker_state_path=/data/tinker_state.json",
      ]
    networks:
      default:
        aliases:
          - claas-api
    environment:
      - CLAAS_TINKER_API_KEY=${TINKER_API_KEY}
      - WAIT_FOR_BACKEND=0
    volumes:
      - claas-feedback-logs:/app/data/feedback
      - tinker-state:/data
    ports:
      - "${CLAAS_API_PORT:-8080}:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/"]
      interval: 10s
      timeout: 5s
      start_period: 20s
      retries: 20

  init-tinker:
    profiles: [tinker]
    build:
      context: ..
      dockerfile: docker/Dockerfile.init
    command:
      [
        "--config-name",
        "tinker",
        "--base-model=${MODEL}",
        "--vllm-base-url=http://claas-api:8080/v1",
        "--lora-root=/loras",
      ]
    environment:
      - LORA_NAME=${LORA_NAME}
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN}
      - OPENCLAW_HOME=/openclaw-config
      - CLAAS_API_URL=http://claas-api:8080
      - FEEDBACK_BATCH_SIZE=${FEEDBACK_BATCH_SIZE}
    volumes:
      - openclaw-config:/openclaw-config
      - ../plugins/claas-feedback:/app/plugins/claas-feedback:ro
    depends_on:
      claas-api-tinker:
        condition: service_healthy

  openclaw-tinker:
    profiles: [tinker]
    build:
      context: ..
      dockerfile: docker/Dockerfile.openclaw
    environment:
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN:-}
      - VLLM_BASE_URL=http://claas-api:8080
      - BACKEND_HEALTH_URL=http://claas-api:8080/v1/models
    volumes:
      - openclaw-config:/home/node/.openclaw
    ports:
      - "${OPENCLAW_PORT:-18789}:18789"
    depends_on:
      claas-api-tinker:
        condition: service_healthy
      init-tinker:
        condition: service_completed_successfully

volumes:
  hf-cache:
  lora-storage:
  openclaw-config:
  claas-feedback-logs:
  tinker-state:
